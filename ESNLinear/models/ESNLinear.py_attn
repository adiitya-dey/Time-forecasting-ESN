from re import X
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
# from ..layers.Embed import PositionalEmbedding

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class PositionwiseFeedForward(nn.Module):
    "Implements FFN equation."
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))

class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean


class ESN(nn.Module):

    def __init__(self, 
                 input_size: int,
                #  output_size: int,
                 reservoir_size = 50,
                 activation = nn.Tanh(),
                 *args, **kwargs) -> None:
        super(ESN, self).__init__(*args, **kwargs)

        self.input_size = input_size
        self.reservoir_size = reservoir_size
        self.activation = activation


        ## Initialize initial state as a Zero Vector.
        self.state = torch.zeros(self.reservoir_size, 1)

         ## Initialize Input Weights as randomly distributed [-1,1].
        w_p = torch.empty(self.reservoir_size, self.reservoir_size)
        P = nn.init.orthogonal_(w_p)

        w_b = torch.empty(self.reservoir_size, self.input_size)
        B = nn.init.uniform_(w_b, a=-1.0, b=1.0)

        self.W_in =  B


        ##Initialize Reservoir Weight matrix.
        w_d =  torch.empty(self.reservoir_size)
        d = nn.init.uniform_(w_d, a=-0.5, b=0.5)
        self.W_res = torch.diag(d)


    ## Calculate states for all inputs.
    def update_state(self, X):
        # batch, segments, windown_len  = X.shape
        all_states = torch.empty(X.shape[0], X.shape[1], self.reservoir_size,)

        for b in range(X.shape[0]):
            for t in range(X.shape[1]):
                self.state = self.get_state(X[b, t, :])
                all_states[b,t, :] = self.state.clone().detach().squeeze(1)
        return all_states

        # for i in range(X.shape[0]):
        #     self.state = self.get_state(X[i])
        #     all_states = torch.vstack([all_states, self.state.clone().detach().squeeze(-1)])
        # return all_states


    ## Reset all states for next batch operation.
    def reset_states(self):
        self.state = torch.zeros(self.reservoir_size, 1)

                    
    ## Calculate state.
    def get_state(self, input):
        input = torch.unsqueeze(input, 1)
        return self.activation(self.W_in@input + self.W_res@self.state)

    def forward(self, X):
        states = self.update_state(X)
        return states



class Model(nn.Module):
    """
    DLinear
    """
    def __init__(self, configs):
        super(Model, self).__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len
        self.window_len = 48
        self.reservoir_size = 50

        # Decompsition Kernel Size
        kernel_size = 25
        self.decompsition = series_decomp(kernel_size)
        self.individual = configs.individual
        self.channels = configs.enc_in


        
        
        self.esn_layer1 = ESN(reservoir_size=self.reservoir_size,
                                            activation=nn.LeakyReLU(1.0),
                                            input_size=1)
        
        self.Q_layer = nn.Linear(in_features=self.reservoir_size,
                           out_features=self.reservoir_size,
                           bias=False)
        
        # self.Q_layer.weight.requires_grad_ = False
        
        self.K_layer = nn.Linear(in_features=self.reservoir_size,
                           out_features=self.reservoir_size,
                           bias=False)
        
        # self.K_layer.weight.requires_grad_ = False
        
        self.V_layer = nn.Linear(in_features=self.reservoir_size,
                           out_features=self.reservoir_size,
                           bias=False)
        # self.V_layer.weight.requires_grad_ = False


        self.attention = nn.MultiheadAttention(embed_dim=self.reservoir_size,
                                               num_heads=5,
                                               )
        self.ffn = PositionwiseFeedForward(d_model=self.reservoir_size, d_ff=100)


        self.layer_norm1 = nn.LayerNorm(self.reservoir_size)
        self.layer_norm2 = nn.LayerNorm(self.reservoir_size)

        self.output_linear_layer = nn.Linear(in_features=336*self.reservoir_size,
                                                 out_features=self.pred_len,
                                                 bias=True)


       
    def forward(self, x):
        # x: [Batch, Input length, Channel]
        batch, seq_len, channel = x.shape

        # Apply ESN layer
        x = self.esn_layer1(x)

        # Apply linear layers to get Q, K, V
        query = self.Q_layer(x)
        key = self.K_layer(x)
        value = self.V_layer(x)

        # Apply multi-head attention
        attn_output, _ = self.attention(query, key, value)

        # Add residual connection
        x = x + attn_output

        # Apply layer normalization
        x = self.layer_norm1(x.permute(1, 0, 2)).permute(1, 0, 2)

        # Apply position-wise feed-forward network
        ffn_output = self.ffn(x)

        # Add residual connection
        x = x + ffn_output

        # Apply layer normalization
        x = self.layer_norm2(x.permute(1, 0, 2)).permute(1, 0, 2)

        # Flattening of batch sequences
        x = x.reshape(x.shape[0], -1)

        # Trainable Prediction layer
        x = self.output_linear_layer(x)

        # Add Channel to dimension
        x = torch.unsqueeze(x, 1)

        return x.permute(0, 2, 1)  # to [Batch, Output length, Channel]
    
    def reset(self):
        self.esn_layer1.reset_states()
        